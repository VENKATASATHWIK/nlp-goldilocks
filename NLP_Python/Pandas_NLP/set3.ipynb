{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9cec645",
   "metadata": {},
   "source": [
    "###  tokenization + aggregation + advanced regex â€” this is where text becomes features for models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e49afee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "data = {\n",
    "    \"id\": [1, 2, 3, 4, 5, 6],\n",
    "    \"category\": [\"tech\", \"tech\", \"sports\", \"sports\", \"news\", \"news\"],\n",
    "    \"text\": [\n",
    "        \"NLP is amazing for AI.\",\n",
    "        \"Transformers dominate NLP benchmarks.\",\n",
    "        \"Football match on 2023-05-14!\",\n",
    "        \"Cricket WorldCup 2024 schedule announced.\",\n",
    "        \"Breaking news: Email leaks admin@domain.com\",\n",
    "        \"Protests continue!!! Huge crowd in New York.\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d492256a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>tech</td>\n",
       "      <td>NLP is amazing for AI.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>tech</td>\n",
       "      <td>Transformers dominate NLP benchmarks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>sports</td>\n",
       "      <td>Football match on 2023-05-14!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>sports</td>\n",
       "      <td>Cricket WorldCup 2024 schedule announced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>news</td>\n",
       "      <td>Breaking news: Email leaks admin@domain.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>news</td>\n",
       "      <td>Protests continue!!! Huge crowd in New York.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id category                                          text\n",
       "0   1     tech                        NLP is amazing for AI.\n",
       "1   2     tech         Transformers dominate NLP benchmarks.\n",
       "2   3   sports                 Football match on 2023-05-14!\n",
       "3   4   sports     Cricket WorldCup 2024 schedule announced.\n",
       "4   5     news   Breaking news: Email leaks admin@domain.com\n",
       "5   6     news  Protests continue!!! Huge crowd in New York."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df  = pd.DataFrame(data)\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ff1615c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                          text  \\\n",
      "0   1                        NLP is amazing for AI.   \n",
      "1   2         Transformers dominate NLP benchmarks.   \n",
      "2   3                 Football match on 2023-05-14!   \n",
      "3   4     Cricket WorldCup 2024 schedule announced.   \n",
      "4   5   Breaking news: Email leaks admin@domain.com   \n",
      "5   6  Protests continue!!! Huge crowd in New York.   \n",
      "\n",
      "                                             tokens  \n",
      "0                       [nlp, is, amazing, for, ai]  \n",
      "1         [transformers, dominate, nlp, benchmarks]  \n",
      "2                   [football, match, on, 20230514]  \n",
      "3    [cricket, worldcup, 2024, schedule, announced]  \n",
      "4    [breaking, news, email, leaks, admindomaincom]  \n",
      "5  [protests, continue, huge, crowd, in, new, york]  \n"
     ]
    }
   ],
   "source": [
    "# Example 31: Tokenize text column\n",
    "import re \n",
    "df[\"tokens\"] =[re.sub(r'[^\\w\\s]',\"\",text.lower()).split() for text in df[\"text\"]]\n",
    "print(df[[\"id\",\"text\",\"tokens\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "121b5e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id category                                          text          tokens\n",
      "0   1     tech                        NLP is amazing for AI.             nlp\n",
      "0   1     tech                        NLP is amazing for AI.              is\n",
      "0   1     tech                        NLP is amazing for AI.         amazing\n",
      "0   1     tech                        NLP is amazing for AI.             for\n",
      "0   1     tech                        NLP is amazing for AI.              ai\n",
      "1   2     tech         Transformers dominate NLP benchmarks.    transformers\n",
      "1   2     tech         Transformers dominate NLP benchmarks.        dominate\n",
      "1   2     tech         Transformers dominate NLP benchmarks.             nlp\n",
      "1   2     tech         Transformers dominate NLP benchmarks.      benchmarks\n",
      "2   3   sports                 Football match on 2023-05-14!        football\n",
      "2   3   sports                 Football match on 2023-05-14!           match\n",
      "2   3   sports                 Football match on 2023-05-14!              on\n",
      "2   3   sports                 Football match on 2023-05-14!        20230514\n",
      "3   4   sports     Cricket WorldCup 2024 schedule announced.         cricket\n",
      "3   4   sports     Cricket WorldCup 2024 schedule announced.        worldcup\n",
      "3   4   sports     Cricket WorldCup 2024 schedule announced.            2024\n",
      "3   4   sports     Cricket WorldCup 2024 schedule announced.        schedule\n",
      "3   4   sports     Cricket WorldCup 2024 schedule announced.       announced\n",
      "4   5     news   Breaking news: Email leaks admin@domain.com        breaking\n",
      "4   5     news   Breaking news: Email leaks admin@domain.com            news\n",
      "4   5     news   Breaking news: Email leaks admin@domain.com           email\n",
      "4   5     news   Breaking news: Email leaks admin@domain.com           leaks\n",
      "4   5     news   Breaking news: Email leaks admin@domain.com  admindomaincom\n",
      "5   6     news  Protests continue!!! Huge crowd in New York.        protests\n",
      "5   6     news  Protests continue!!! Huge crowd in New York.        continue\n",
      "5   6     news  Protests continue!!! Huge crowd in New York.            huge\n",
      "5   6     news  Protests continue!!! Huge crowd in New York.           crowd\n",
      "5   6     news  Protests continue!!! Huge crowd in New York.              in\n",
      "5   6     news  Protests continue!!! Huge crowd in New York.             new\n",
      "5   6     news  Protests continue!!! Huge crowd in New York.            york\n"
     ]
    }
   ],
   "source": [
    "# Example 32: Explode tokens into separate rows\n",
    "df_exploded = df.explode(\"tokens\")\n",
    "print(df_exploded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46a96568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens\n",
      "nlp               2\n",
      "schedule          1\n",
      "new               1\n",
      "in                1\n",
      "crowd             1\n",
      "huge              1\n",
      "continue          1\n",
      "protests          1\n",
      "admindomaincom    1\n",
      "leaks             1\n",
      "email             1\n",
      "news              1\n",
      "breaking          1\n",
      "announced         1\n",
      "2024              1\n",
      "is                1\n",
      "worldcup          1\n",
      "cricket           1\n",
      "20230514          1\n",
      "on                1\n",
      "match             1\n",
      "football          1\n",
      "benchmarks        1\n",
      "dominate          1\n",
      "transformers      1\n",
      "ai                1\n",
      "for               1\n",
      "amazing           1\n",
      "york              1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Example 33: Count token frequency\n",
    "\n",
    "token_freq = df_exploded[\"tokens\"].value_counts()\n",
    "print(token_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "743022e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id category                                   text tokens\n",
      "0   1     tech                 NLP is amazing for AI.    nlp\n",
      "1   2     tech  Transformers dominate NLP benchmarks.    nlp\n"
     ]
    }
   ],
   "source": [
    "# Example 34: Remove rare tokens (appear once)\n",
    "rare_tokens = token_freq[token_freq==1].index\n",
    "df_filtered = df_exploded[~df_exploded[\"tokens\"].isin(rare_tokens)]\n",
    "\n",
    "print(df_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "515ec53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n",
      "news      12\n",
      "sports     9\n",
      "tech       9\n",
      "Name: word_count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 35 \n",
    "df[\"word_count\"] = df[\"text\"].str.split().str.len()\n",
    "avg_word_count = df.groupby(\"category\")[\"word_count\"].sum()\n",
    "print(avg_word_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e9ce7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n",
      "news      Breaking news: Email leaks admin@domain.com Pr...\n",
      "sports    Football match on 2023-05-14! Cricket WorldCup...\n",
      "tech      NLP is amazing for AI. Transformers dominate N...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Example 36: Group by category and concatenate all texts\n",
    "all_text_by_cat = df.groupby(\"category\")[\"text\"].apply(lambda x:\" \".join(x))\n",
    "print(all_text_by_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03526fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n",
      "news      2\n",
      "sports    2\n",
      "tech      2\n",
      "Name: id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Example 37: Group by category and count messages\n",
    "msg_count_by_cat = df.groupby(\"category\")[\"id\"].count()\n",
    "print(msg_count_by_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f90d4c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>tech</td>\n",
       "      <td>NLP is amazing for AI.</td>\n",
       "      <td>[nlp, is, amazing, for, ai]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>tech</td>\n",
       "      <td>Transformers dominate NLP benchmarks.</td>\n",
       "      <td>[transformers, dominate, nlp, benchmarks]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>sports</td>\n",
       "      <td>Football match on 2023-05-14!</td>\n",
       "      <td>[football, match, on, 20230514]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>sports</td>\n",
       "      <td>Cricket WorldCup 2024 schedule announced.</td>\n",
       "      <td>[cricket, worldcup, 2024, schedule, announced]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>news</td>\n",
       "      <td>Breaking news: Email leaks admin@domain.com</td>\n",
       "      <td>[breaking, news, email, leaks, admindomaincom]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>news</td>\n",
       "      <td>Protests continue!!! Huge crowd in New York.</td>\n",
       "      <td>[protests, continue, huge, crowd, in, new, york]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id category                                          text  \\\n",
       "0   1     tech                        NLP is amazing for AI.   \n",
       "1   2     tech         Transformers dominate NLP benchmarks.   \n",
       "2   3   sports                 Football match on 2023-05-14!   \n",
       "3   4   sports     Cricket WorldCup 2024 schedule announced.   \n",
       "4   5     news   Breaking news: Email leaks admin@domain.com   \n",
       "5   6     news  Protests continue!!! Huge crowd in New York.   \n",
       "\n",
       "                                             tokens  word_count  \n",
       "0                       [nlp, is, amazing, for, ai]           5  \n",
       "1         [transformers, dominate, nlp, benchmarks]           4  \n",
       "2                   [football, match, on, 20230514]           4  \n",
       "3    [cricket, worldcup, 2024, schedule, announced]           5  \n",
       "4    [breaking, news, email, leaks, admindomaincom]           5  \n",
       "5  [protests, continue, huge, crowd, in, new, york]           7  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab154385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  mentions\n",
      "0   1       NaN\n",
      "1   2       NaN\n",
      "2   3       NaN\n",
      "3   4       NaN\n",
      "4   5  [domain]\n",
      "5   6       NaN\n"
     ]
    }
   ],
   "source": [
    "# Example 38: Extract mentions (@username)\n",
    "df[\"mentions\"] = df[\"text\"].str.extractall(r'@(\\w+)').groupby(level=0)[0].agg(list)\n",
    "print(df[[\"id\", \"mentions\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0eb493ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id       dates\n",
      "0   1         NaN\n",
      "1   2         NaN\n",
      "2   3  2023-05-14\n",
      "3   4        2024\n",
      "4   5         NaN\n",
      "5   6         NaN\n"
     ]
    }
   ],
   "source": [
    "## Example 39: Extract dates (YYYY-MM-DD or YYYY format)\n",
    "df['dates'] = df['text'].str.extract(r'(\\d{4}(?:-\\d{2}-\\d{2})?)')\n",
    "print(df[[\"id\",\"dates\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fc379a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  high_exclaim\n",
      "0   1         False\n",
      "1   2         False\n",
      "2   3         False\n",
      "3   4         False\n",
      "4   5         False\n",
      "5   6          True\n"
     ]
    }
   ],
   "source": [
    "# Example 40: Flag texts with >3 exclamation marks\n",
    "\n",
    "df[\"high_exclaim\"] = df[\"text\"].str.contains(r'!{3,}')\n",
    "print(df[[\"id\",\"high_exclaim\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d455f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
